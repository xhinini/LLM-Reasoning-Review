# 🧠 LLM Reasoning Review

A curated collection of research papers focused on the **reasoning capabilities of Large Language Models (LLMs)**. This repository organizes and categorizes works that evaluate, benchmark, analyze, and summarize reasoning techniques used in LLMs.

The goal is to support researchers, engineers, and enthusiasts in exploring the state of the art and emerging trends in LLM-based reasoning.

---

## 🗂 Categories

- 📚 **Surveys & Meta-Analyses**
  - Literature reviews and taxonomies of LLM reasoning work
- 🔍 **Evaluation & Benchmarking**
  - Performance comparisons across tasks and models
  - New benchmarks and metrics
- 🧠 **Reasoning Techniques**
  - Chain-of-Thought prompting
  - Self-consistency, scratchpads, tool-augmented reasoning, etc.
---

## 📌 Paper List

## Survey

- **[Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey](https://arxiv.org/abs/2503.12605), [[code]](https://github.com/yaotingwangofficial/Awesome-MCoT)**  
  *Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, Hao Fei* • *ICSE 2025*  
  📁 Tags: `evaluation`, `chain-of-thought`, `survey`  
 <!-- 📝 Summary: One or two sentences describing the paper's contribution.-->

## Reasoning-quality

- **[Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases](https://arxiv.org/abs/2503.04691), [[code]](https://github.com/MAGIC-AI4Med/MedRBench)**  
  *Pengcheng Qiu, Chaoyi Wu, Shuyu Liu, Weike Zhao, Zhuoxia Chen, Hongfei Gu, Chuanjin Peng, Ya Zhang, Yanfeng Wang, Weidi Xie* • *ICSE 2025*  
  📁 Tags: `evaluation`, `reasoning-quality`, `reasoning-quality`  
  <!-- 📝 Summary: One or two sentences describing the paper's contribution. -->

